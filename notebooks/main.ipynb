{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionGS Colab Playbook\n",
    "\n",
    "This notebook consolidates the data prep, configuration, training, and evaluation steps for running DiffusionGS in Google Colab. The cells mirror the commands from the README so you can run end-to-end experiments without modifying the source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements and variables\n",
    "\n",
    "Set these environment variables/arguments before running the heavy cells:\n",
    "\n",
    "* `PROJECT_ROOT`: path where the repo is checked out (default: `/content/Open-DiffusionGS`).\n",
    "* `RAW_DATA_DIR`: folder containing unzipped RealEstate10K zips.\n",
    "* `PROCESSED_DIR`: output folder for processed RealEstate10K data (`full_list.txt` lives here).\n",
    "* `OBJ_JSON_DIR`: folder with Objaverse JSON splits (`train.json`, `val.json`, `test.json`).\n",
    "* `OBJ_IMAGE_DIR`: Objaverse image root (e.g., extracted `gobjaverse` folder).\n",
    "* `OUTPUT_DIR`: root folder to collect checkpoints, renders, and logs (often a Drive path).\n",
    "* `HF_TOKEN`: optional Hugging Face token if you host splits/checkpoints privately.\n",
    "\n",
    "These values can also be exported in Colab via `os.environ[...] = ...` or Colab form fields."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Point to your repo checkout; keep the default on Colab.\n",
    "PROJECT_ROOT = Path(os.getenv(\"PROJECT_ROOT\", \"/content/Open-DiffusionGS\"))\n",
    "RAW_DATA_DIR = Path(os.getenv(\"RAW_DATA_DIR\", \"/content/drive/MyDrive/re10k_raw\"))\n",
    "PROCESSED_DIR = Path(os.getenv(\"PROCESSED_DIR\", \"/content/drive/MyDrive/re10k_processed\"))\n",
    "OBJ_JSON_DIR = Path(os.getenv(\"OBJ_JSON_DIR\", \"/content/drive/MyDrive/gobjaverse/json\"))\n",
    "OBJ_IMAGE_DIR = Path(os.getenv(\"OBJ_IMAGE_DIR\", \"/content/drive/MyDrive/gobjaverse\"))\n",
    "OUTPUT_DIR = Path(os.getenv(\"OUTPUT_DIR\", \"/content/drive/MyDrive/diffusiongs\"))\n",
    "\n",
    "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"RAW_DATA_DIR: {RAW_DATA_DIR}\")\n",
    "print(f\"PROCESSED_DIR: {PROCESSED_DIR}\")\n",
    "print(f\"OBJ_JSON_DIR: {OBJ_JSON_DIR}\")\n",
    "print(f\"OBJ_IMAGE_DIR: {OBJ_IMAGE_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab quickstart (no code edits)\n",
    "\n",
    "1. Run the Drive mount cell below.\n",
    "2. Run the clone/setup cell to fetch the repo into `PROJECT_ROOT`.\n",
    "3. Run the dependency install cell (first session only).\n",
    "4. Fill in the dataset paths (or enable the optional download toggles).\n",
    "5. Choose continuous vs. discrete+octree configs and launch training.\n",
    "6. Run inference and the FID/KID cell to score results.\n",
    "7. Save outputs to Drive automatically."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "if [ ! -d \"$PROJECT_ROOT/.git\" ]; then\n",
    "  git clone https://github.com/PRISM-LAB/Open-DiffusionGS.git \"$PROJECT_ROOT\"\n",
    "else\n",
    "  echo 'Repo already present at $PROJECT_ROOT'\n",
    "fi\n",
    "cd \"$PROJECT_ROOT\"\n",
    "git status --short\n",
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)\n",
    "\n",
    "Run this once per session to access data and to save outputs to Drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print('Drive mounted. If your repo is elsewhere, update PROJECT_ROOT accordingly.')\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    print('Not running inside Colab or Drive mount failed:', exc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies (first run)\n",
    "\n",
    "Uncomment the `%pip` line the first time you open the notebook to install project requirements in Colab."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "if os.getenv('SKIP_PIP_INSTALL', '0').lower() not in {'1','true'}:\n",
    "    !pip install -r requirement.txt\n",
    "else:\n",
    "    print('Skipping dependency install because SKIP_PIP_INSTALL is set.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm working directory\n",
    "Ensures Python paths point to the cloned repo before running preprocessing or training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print('CWD ->', Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RealEstate10K preprocessing\n",
    "\n",
    "Convert the downloaded RealEstate10K zips into the `train/full_list.txt` and `test/full_list.txt` files expected by the trainer. The commands mirror the README."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%bash\n",
    "cd \"$PROJECT_ROOT\"\n",
    "python process_data.py --base_path \"$RAW_DATA_DIR\" --output_dir \"$PROCESSED_DIR\" --mode train\n",
    "python process_data.py --base_path \"$RAW_DATA_DIR\" --output_dir \"$PROCESSED_DIR\" --mode test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_list = Path(PROCESSED_DIR) / \"train/full_list.txt\"\n",
    "test_list = Path(PROCESSED_DIR) / \"test/full_list.txt\"\n",
    "\n",
    "for name, path in (('train', train_list), ('test', test_list)):\n",
    "    if path.exists():\n",
    "        print(f\"{name} list: {path}\")\n",
    "        with path.open() as fp:\n",
    "            sample = [fp.readline().strip() for _ in range(5)]\n",
    "        print(\"Sample entries:\", [s for s in sample if s])\n",
    "    else:\n",
    "        print(f\"Missing {name} list at {path}; ensure preprocessing succeeded.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objaverse splits and assets\n",
    "\n",
    "Set the JSON splits and image directory. You can symlink existing Drive folders or download public splits if you host them yourself."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Toggle to fetch remote JSON splits you host (set OBJ_JSON_URL accordingly).\n",
    "DOWNLOAD_JSON_SPLITS = False\n",
    "OBJ_JSON_URL = os.getenv(\"OBJ_JSON_URL\", \"\")  # e.g., a Hugging Face file URL if you mirror the splits.\n",
    "LOCAL_JSON_ARCHIVE = OUTPUT_DIR / \"objaverse_json.tar.gz\"\n",
    "\n",
    "OBJ_JSON_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OBJ_IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DOWNLOAD_JSON_SPLITS and OBJ_JSON_URL:\n",
    "    !wget -O \"$LOCAL_JSON_ARCHIVE\" \"$OBJ_JSON_URL\"\n",
    "    !tar -xzf \"$LOCAL_JSON_ARCHIVE\" -C \"$OBJ_JSON_DIR\"\n",
    "else:\n",
    "    print(\"Skipping download; using existing OBJ_JSON_DIR and OBJ_IMAGE_DIR.\")\n",
    "print(\"JSON dir:\", OBJ_JSON_DIR)\n",
    "print(\"Image dir:\", OBJ_IMAGE_DIR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShapeNet category subsets (optional)\n",
    "\n",
    "If you want to fine-tune on specific ShapeNet synsets, set `USE_SHAPENET_CONFIG=True` and provide a category mapping JSON plus a comma-separated category list in `SHAPENET_CATEGORY_FILTER`. You can keep everything in Drive and run entirely from this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally switch to the ShapeNet-specific config and filter by categories\n",
    "USE_SHAPENET_CONFIG = False  # Set True to start from diffusionGS/configs/diffusionGS_shapenet.yaml\n",
    "SHAPENET_CATEGORY_MAP = os.getenv(\"SHAPENET_CATEGORY_MAP\", str(PROJECT_ROOT / \"examp_data/shapenet_category_map.example.json\"))\n",
    "SHAPENET_CATEGORY_FILTER = os.getenv(\"SHAPENET_CATEGORY_FILTER\", \"\")\n",
    "SHAPENET_CATEGORY_KEY = os.getenv(\"SHAPENET_CATEGORY_KEY\", \"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure datamodule and scheduler options\n",
    "\n",
    "Flip `DISCRETE_TOKENIZE` / `DISCRETE_OCTREE_DEPTH` before writing the final config. If you want to use the discrete scheduler, set `USE_DISCRETE_SCHEDULER=True` to override `system.noise_scheduler_type`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "default_config = \"diffusionGS/configs/diffusionGS_shapenet.yaml\" if USE_SHAPENET_CONFIG else \"diffusionGS/configs/diffusionGS_rel.yaml\"\n",
    "CONFIG_PATH = os.getenv(\"CONFIG_PATH\", str(PROJECT_ROOT / default_config))\n",
    "TMP_CONFIG_PATH = str(PROJECT_ROOT / \"notebooks/colab_config.yaml\")\n",
    "DISCRETE_TOKENIZE = False\n",
    "DISCRETE_OCTREE_DEPTH = 3\n",
    "USE_DISCRETE_SCHEDULER = True\n",
    "\n",
    "cfg = OmegaConf.load(CONFIG_PATH)\n",
    "\n",
    "# Point to Objaverse or ShapeNet assets; fall back to notebook paths when placeholders are detected\n",
    "if not cfg.data.get('local_dir') or 'DATAPATH' in str(cfg.data.local_dir):\n",
    "    cfg.data.local_dir = str(OBJ_JSON_DIR)\n",
    "if not cfg.data.get('image_dir') or 'DATAPATH' in str(cfg.data.image_dir):\n",
    "    cfg.data.image_dir = str(OBJ_IMAGE_DIR)\n",
    "\n",
    "cfg.data.local_eval_dir = cfg.data.get('local_eval_dir', str(test_list))\n",
    "\n",
    "# Toggle discrete tokenization\n",
    "cfg.data.discrete_tokenize = DISCRETE_TOKENIZE\n",
    "cfg.data.discrete_octree_depth = DISCRETE_OCTREE_DEPTH\n",
    "\n",
    "# Optional ShapeNet category filtering\n",
    "shapenet_categories = [c.strip() for c in SHAPENET_CATEGORY_FILTER.split(',') if c.strip()]\n",
    "if USE_SHAPENET_CONFIG or shapenet_categories:\n",
    "    cfg.data.category_mapping_json = str(SHAPENET_CATEGORY_MAP)\n",
    "    cfg.data.category_filter = shapenet_categories or cfg.data.get('category_filter', [])\n",
    "    cfg.data.category_key = SHAPENET_CATEGORY_KEY\n",
    "\n",
    "if USE_DISCRETE_SCHEDULER:\n",
    "    cfg.system.noise_scheduler_type = \"diffusionGS.models.scheduler.discrete_scheduler.DiscreteScheduler\"\n",
    "\n",
    "# Save a copy so launch.py can consume it directly\n",
    "Path(TMP_CONFIG_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "OmegaConf.save(cfg, TMP_CONFIG_PATH)\n",
    "print(\"Wrote config to\", TMP_CONFIG_PATH)\n",
    "print(OmegaConf.to_yaml(cfg))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toggle continuous vs. octree-token configs\n",
    "\n",
    "Choose whether to run the discrete octree-tokenized pipeline or fall back to a continuous scheduler without editing the YAML by hand."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate separate configs for octree tokens and continuous scheduling\n",
    "RUN_WITH_OCTREE_TOKENS = True  # Set False to compare against the continuous scheduler.\n",
    "\n",
    "OCTREE_CONFIG_PATH = str(PROJECT_ROOT / 'notebooks/colab_config_octree.yaml')\n",
    "CONTINUOUS_CONFIG_PATH = str(PROJECT_ROOT / 'notebooks/colab_config_continuous.yaml')\n",
    "\n",
    "# Preserve the base config overrides set above, then branch into discrete/continuous variants.\n",
    "base_cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n",
    "\n",
    "octree_cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n",
    "octree_cfg.data.discrete_tokenize = True\n",
    "octree_cfg.data.discrete_octree_depth = DISCRETE_OCTREE_DEPTH\n",
    "octree_cfg.system.noise_scheduler_type = 'diffusionGS.models.scheduler.discrete_scheduler.DiscreteScheduler'\n",
    "OmegaConf.save(octree_cfg, OCTREE_CONFIG_PATH)\n",
    "\n",
    "continuous_cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n",
    "continuous_cfg.data.discrete_tokenize = False\n",
    "continuous_cfg.system.noise_scheduler_type = 'diffusionGS.models.scheduler.ddim_scheduler.DDIMScheduler'\n",
    "OmegaConf.save(continuous_cfg, CONTINUOUS_CONFIG_PATH)\n",
    "\n",
    "TMP_CONFIG_PATH = OCTREE_CONFIG_PATH if RUN_WITH_OCTREE_TOKENS else CONTINUOUS_CONFIG_PATH\n",
    "print('Discrete octree-token config:', OCTREE_CONFIG_PATH)\n",
    "print('Continuous scheduler config:', CONTINUOUS_CONFIG_PATH)\n",
    "print('Using config ->', TMP_CONFIG_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Lightning modules\n",
    "\n",
    "Instantiates the datamodule and system using the saved config. This is useful for checking that dataset paths and scheduler choices resolve correctly before training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import diffusionGS\n",
    "from diffusionGS.utils.config import load_config\n",
    "\n",
    "cfg_struct = load_config(TMP_CONFIG_PATH, n_gpus=1)\n",
    "\n",
    "datamodule = diffusionGS.find(cfg_struct.data_type)(cfg_struct.data)\n",
    "system = diffusionGS.find(cfg_struct.system_type)(cfg_struct.system, resumed=cfg_struct.resume is not None)\n",
    "system.set_save_dir(str(OUTPUT_DIR / \"save\"))\n",
    "\n",
    "print(\"Datamodule:\", datamodule)\n",
    "print(\"System:\", system.__class__.__name__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: pass octree tokens to the scheduler/model\n",
    "\n",
    "Pulls a small batch to feed `octree_tokens_input` into the scheduler so the diffusion model conditions on the octree tokens when they are available."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if cfg_struct.data.discrete_tokenize and train_list.exists():\n",
    "    try:\n",
    "        sample_batch = next(iter(datamodule.train_dataloader()))\n",
    "        octree_tokens = sample_batch.get('octree_tokens_input')\n",
    "        if octree_tokens is None:\n",
    "            print('Batch missing octree_tokens_input; double-check datamodule settings.')\n",
    "        else:\n",
    "            if hasattr(system.noise_scheduler, 'set_conditioning_tokens'):\n",
    "                system.noise_scheduler.set_conditioning_tokens(octree_tokens)\n",
    "            print('Octree tokens passed to scheduler/model:', octree_tokens.shape)\n",
    "    except Exception as exc:\n",
    "        print('Could not fetch a batch to attach octree tokens:', exc)\n",
    "elif cfg_struct.data.discrete_tokenize:\n",
    "    print(f'Skipping token pass-through because {train_list} is missing.')\n",
    "else:\n",
    "    print('Discrete tokenization disabled; using the continuous path.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (stage 1 / stage 2)\n",
    "\n",
    "Adjust `--gpu` to match your Colab runtime. Stage 2 should reuse the stage-1 checkpoint via `shape_model.pretrained_model_name_or_path` in the config."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%bash\n",
    "cd \"$PROJECT_ROOT\"\n",
    "# Stage 1 (e.g., 256px)\n",
    "python launch.py --config \"$TMP_CONFIG_PATH\" --gpu 0\n",
    "# Stage 2 example (update TMP_CONFIG_PATH to point to a stage-2 config with pretrained path set)\n",
    "# python launch.py --config diffusionGS/configs/diffusionGS_rel_512.yaml --gpu 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference / evaluation\n",
    "\n",
    "Render test scenes or objects, then copy outputs to Drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%bash\n",
    "cd \"$PROJECT_ROOT\"\n",
    "CHECKPOINT_PATH=\"$OUTPUT_DIR/save/last.ckpt\"\n",
    "RENDER_DIR=\"$OUTPUT_DIR/renders\"\n",
    "mkdir -p \"$RENDER_DIR\"\n",
    "python eval_scene_result.py --path \"$CHECKPOINT_PATH\" --chunk 64 --out \"$RENDER_DIR\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "render_dir = OUTPUT_DIR / \"renders\"\n",
    "backup_dir = OUTPUT_DIR / \"renders_backup\"\n",
    "backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "for item in render_dir.glob(\"*\"):\n",
    "    target = backup_dir / item.name\n",
    "    if item.is_file():\n",
    "        shutil.copy(item, target)\n",
    "    elif item.is_dir():\n",
    "        shutil.copytree(item, target, dirs_exist_ok=True)\n",
    "print(\"Copied renders to\", backup_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FID/KID (optional)\n",
    "\n",
    "After rendering samples, point the metric script at a folder of generated images and a folder of references."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd \"$PROJECT_ROOT\"\n",
    "REFERENCE_DIR=\"$OUTPUT_DIR/reference_images\"  # update to your ground-truth images\n",
    "GENERATED_DIR=\"$OUTPUT_DIR/renders\"         # generated renders from the inference cell\n",
    "python scripts/compute_fid_kid.py   --reference \"$REFERENCE_DIR\"   --generated \"$GENERATED_DIR\"   --batch-size 8 --image-size 299 --kid-subset-size 100   --output \"$OUTPUT_DIR/metrics.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick visualization (images or video)\n",
    "\n",
    "Display a small grid or video preview from the rendered outputs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "image_paths = sorted((OUTPUT_DIR / \"renders\").glob(\"*.png\"))[:4]\n",
    "if image_paths:\n",
    "    ncols = len(image_paths)\n",
    "    fig, axes = plt.subplots(1, ncols, figsize=(4 * ncols, 4))\n",
    "    for ax, img_path in zip(axes, image_paths):\n",
    "        ax.imshow(Image.open(img_path))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(img_path.name)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No PNG renders found; run the inference cell first.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}